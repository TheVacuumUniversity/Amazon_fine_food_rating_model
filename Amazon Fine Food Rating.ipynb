{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libraries\" data-toc-modified-id=\"Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libraries</a></span></li><li><span><a href=\"#MongoDb\" data-toc-modified-id=\"MongoDb-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>MongoDb</a></span></li><li><span><a href=\"#Data-load-from-a-file\" data-toc-modified-id=\"Data-load-from-a-file-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data load from a file</a></span></li><li><span><a href=\"#Data-transformation\" data-toc-modified-id=\"Data-transformation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data transformation</a></span></li><li><span><a href=\"#Data-discovery\" data-toc-modified-id=\"Data-discovery-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Data discovery</a></span></li><li><span><a href=\"#Training-and-test-set\" data-toc-modified-id=\"Training-and-test-set-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Training and test set</a></span></li><li><span><a href=\"#Featured-words\" data-toc-modified-id=\"Featured-words-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Featured words</a></span></li><li><span><a href=\"#NLTK-library---The-Naive-bayes-model\" data-toc-modified-id=\"NLTK-library---The-Naive-bayes-model-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>NLTK library - The Naive bayes model</a></span></li><li><span><a href=\"#Accuracy---Confusion-matrix\" data-toc-modified-id=\"Accuracy---Confusion-matrix-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Accuracy - Confusion matrix</a></span></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Next-steps\" data-toc-modified-id=\"Next-steps-11\"><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Next steps</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon fine food - rating prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have Amazon data about part of their product. In this section we are going to predict STAR rating according to sentences that a user wrote into summary and text field. See example https://www.amazon.com/Apple-Watch-Gold-Aluminum-Sport/dp/B075TDXYCS/ref=sr_1_17?s=electronics&ie=UTF8&qid=1540892072&sr=1-17#customerReviews \n",
    "\n",
    "\n",
    "Model selection\n",
    "\n",
    "We deceided to use Naive Bayes model as a good example of bayes classifiers. Naive Bayes classifiers mostly used in text classification (due to better result in multi class problems and independence rule) have higher success rate as compared to other algorithms. As a result, it is widely used in Spam filtering. It perform well in case of categorical input variables compared to numerical variable(s). The Naive Bayes classifier performs better compare to other models like logistic regression and you need less training data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient #necessary to locally install mongodb \n",
    "from pymongo import DESCENDING\n",
    "import numpy as np #library with mathematical tools \n",
    "import matplotlib.pyplot as plt #for ploting graphs \n",
    "import pandas as pd #library for manage and import datasets\n",
    "import nltk #natural language processing library containing NaiveBayes\n",
    "from sklearn.model_selection import train_test_split #very useful when splitting a sample\n",
    "from nltk.metrics import ConfusionMatrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MongoDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to upload data from a pandas dataframe to a local MongoDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Auxiliary function - upload to Mongodb\n",
    "\n",
    "def upload_data_mongoDb(collection, data, delete_before_upload = True, silent_mode = False):\n",
    "    try:\n",
    "        if delete_before_upload == True: \n",
    "            # delete before insert\n",
    "            collection.delete_many({})\n",
    "            \n",
    "        # insert the dataframe to mongodb\n",
    "        collection.insert_many(data)\n",
    "\n",
    "        # dataframe_load = []\n",
    "        data = []\n",
    "        if silent_mode == False:\n",
    "            print('Dataframe uploaded to MongoDb')\n",
    "\n",
    "    except:\n",
    "        print('Error occured while uploading data to MongoDb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to localhost MongoDB database\n",
    "client = MongoClient()\n",
    "client = MongoClient('localhost', 27017)\n",
    "\n",
    "# connect to an amazon_database\n",
    "db = client.amazon_database\n",
    "\n",
    "# Collections\n",
    "# connect to an amazon collection in the amazon database\n",
    "collection = db.amazon_collection\n",
    "\n",
    "# connect to an transformed collection in the amazon database \n",
    "transformed_collection = db.transformed_collection\n",
    "\n",
    "# connect to an word features collection in the amazon database \n",
    "wordfeatures_collection = db.wordfeatures_collection\n",
    "\n",
    "# connect to an training set collection in the amazon database \n",
    "train_set_collection = db.train_set_collection\n",
    "\n",
    "# connect to an training set collection in the amazon database - used in nltk model\n",
    "train_set_collection_nltk = db.train_set_collection_nltk\n",
    "\n",
    "# connect to an test set  collection in the amazon database \n",
    "test_set_collection = db.test_set_collection\n",
    "\n",
    "# connect to an test set  collection in the amazon database - used in nltk model\n",
    "test_set_collection_nltk = db.test_set_collection_nltk\n",
    "\n",
    "# connect to an featuring_temporary_collection in the amazon database \n",
    "featuring_temporary_collection = db.featuring_temporary_collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "myColl = {}\n",
    "linecount = 1\n",
    "\n",
    "def makeDataFrame(dictionary):\n",
    "    global df\n",
    "    df=df.append(dictionary,ignore_index=True)      \n",
    "\n",
    "\n",
    "def loadData(filename, loadlines = np.inf):  \n",
    "    global myColl \n",
    "    global linecount\n",
    "    \n",
    "    # Open the file.\n",
    "    f = open(filename, \"r\")\n",
    "\n",
    "    \n",
    "    while(linecount <= loadlines):\n",
    "        try:\n",
    "            # Read a line.\n",
    "            line = f.readline()\n",
    "\n",
    "            # When readline returns an empty string, the file is fully read.\n",
    "            if line == \"\":\n",
    "                makeDataFrame(myColl)\n",
    "                myColl={}\n",
    "                break\n",
    "\n",
    "            # When a newline is returned, the line is empty.\n",
    "            if line == \"\\n\":\n",
    "                linecount = linecount + 1\n",
    "                makeDataFrame(myColl)\n",
    "                myColl={}\n",
    "                continue\n",
    "\n",
    "            # Print other lines.\n",
    "            stripped = line.strip().split(': ')\n",
    "            myColl[stripped[0]]=stripped[1]\n",
    "        except:\n",
    "            print(line)\n",
    "            print(linecount)\n",
    "            continue\n",
    "            \n",
    "    return df\n",
    "\n",
    "# number of loaded lines is limited \n",
    "dataframe = loadData(filename=\"foodscopy.txt\",loadlines = 500000)\n",
    "\n",
    "# number of loaded lines is unlimited - we load the whole sample\n",
    "# dataframe = loadData(filename=\"foodscopy.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Upload data from file to mongoDb\n",
    "upload_data_mongoDb(collection,dataframe.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a auxiliary function for following data transformation. First, we create a function that will remove meaningless word. We use nltk function pos_tag that assign a part of speech to every word. Then, we choose only relevant ones. Last, we can drop off columns that we are not going to use in this run such as productid or userid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove useless part of speech words such as 'the','a','this','that'. Leave only adjectives, nouns \n",
    "# See https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "\n",
    "def remove_wrong_words(wordlist):\n",
    "    wordlist2 = []\n",
    "    wordlist2 = nltk.pos_tag(nltk.word_tokenize(wordlist.replace('.',' ')))\n",
    "#     wordlist4 = [word for (word, pos) in wordlist3 if pos not in ['DT','EX','FW','RP','SYM','TO','IN','CC']] --\n",
    "    wordlist3 = [word for (word, pos) in wordlist2 if pos in ['JJ','JJR','JJS','RB','RBR','RBS','UH','NN','NNS','NNP','NNSP']]\n",
    "    return wordlist3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create new column 'good/bad' from 'review/score' column as well as summary_transformed column where we use remove_wrong_wrong function from previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load data from MongoDb\n",
    "data = pd.DataFrame(list(collection.find()))\n",
    "\n",
    "# pick relevant columns\n",
    "data = data[['review/summary', 'review/text', 'review/score']]\n",
    "data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "data['dependent_variable']  = data['review/score'].apply(lambda x: 1 if float(x) >= 4 else 0)\n",
    "\n",
    "# remove non significant words\n",
    "data['independent_variable'] = data['review/summary'].apply(lambda x: remove_wrong_words(x.lower()))\n",
    "\n",
    "# Choose only independant variable and dependant variable\n",
    "data = data[['independent_variable', 'dependent_variable']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['id'] = data.reset_index().index\n",
    "data.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Upload transformed data to MongoDb\n",
    "upload_data_mongoDb(transformed_collection,data.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data discovery "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for counting good and bad ratings in order to see a ration in our sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def good_count(dataframe):\n",
    "    dataframe['dependent_variable'] = dataframe['dependent_variable'].apply(lambda x: int(x))\n",
    "    print ('Number of good ratings: ' + str(sum(dataframe['dependent_variable'])))\n",
    "    print ('Number of total ratings: ' + str(len(dataframe['dependent_variable'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of good ratings: 389844\n",
      "Number of total ratings: 500000\n"
     ]
    }
   ],
   "source": [
    "data = pd.DataFrame(list(transformed_collection.find({},{\"dependent_variable\"}))) \n",
    "good_count(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apparently, Amazon is selling pretty quality products :) in Amazon fine food section due to high number of positive rating. This may cause a trouble in terms of an accuraccy. Let's keep it in mind and see how many bad rating we have in training sample as well as test sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(list(transformed_collection.find()))\n",
    "data = data.sample(n = 30000)\n",
    "data.set_index('id')\n",
    "\n",
    "# sklearn\n",
    "train_set, test_set = train_test_split(data, test_size=0.7, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_set\n",
      "Number of good ratings: 7012\n",
      "Number of total ratings: 9000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print('training_set')\n",
    "good_count(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_set\n",
      "Number of good ratings: 16326\n",
      "Number of total ratings: 21000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print('test_set')\n",
    "good_count(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Upload training data to MongoDb\n",
    "upload_data_mongoDb(train_set_collection,train_set.to_dict('records'))\n",
    "train_set_collection.create_index([(\"id\", DESCENDING)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Upload test data to MongoDb\n",
    "upload_data_mongoDb(test_set_collection,test_set.to_dict('records'))\n",
    "test_set_collection.create_index([(\"id\", DESCENDING)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featured words "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Naive Bayes we need to create featured words that will indicate good/bad. We want to use only the most common word used in our train sample. We call it 'wordfeatures'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick up only TOP most common word in our dataset and use them as featured words \n",
    "def wordFeatures(wordList,top):\n",
    "    forbidenwords = ['.','..','%',\"n't\",'amazon.com','dr.','mrs.','.but','mr.','tea..']\n",
    "    wordList = nltk.FreqDist(wordList)\n",
    "    wordList = wordList.most_common(top)\n",
    "    wordFeatures = [{'features':words} for words,counts in wordList if words not in forbidenwords]\n",
    "    return wordFeatures   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a list with all word occured in the dataset. We call it features and we upload it to MongoDB into collection wordfeatures_collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordList = []\n",
    "\n",
    "for row in train_set_collection.find({},{\"independent_variable\"}):\n",
    "    wordList.extend(row['independent_variable'])\n",
    "\n",
    "features = wordFeatures(wordList,10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Upload featured word to MongoDb\n",
    "upload_data_mongoDb(wordfeatures_collection,features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK library - The Naive bayes model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare new column independent_variable_naive_bayes into a format that is being used in nltk library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does words in a sentence contains in featured words \n",
    "def getFeatures(doc,featuredwords):\n",
    "    docWords = set(doc)\n",
    "    feat ={}\n",
    "    for word in featuredwords:\n",
    "        feat['contains(%s)' % word] = (word in docWords)\n",
    "    return feat\n",
    "\n",
    "def feature_dataset(collection, featuredwords):\n",
    "    data = []\n",
    "    featuring_temporary_collection.delete_many({})\n",
    "    \n",
    "    for i in range(0,500):\n",
    "        try:\n",
    "            # classsic ETL \n",
    "            number_of_lines = 1000\n",
    "            # load data\n",
    "            data = pd.DataFrame(list(collection.find({\"id\": {\"$gte\": i * number_of_lines , \"$lt\": (i+1)*number_of_lines}})))\n",
    "            # transform data\n",
    "            data['independent_variable_naive_bayes'] = data['independent_variable'].apply(lambda x: getFeatures(x,featuredwords))\n",
    "            # load data       \n",
    "            upload_data_mongoDb(featuring_temporary_collection, \n",
    "                                data.to_dict('records'), \n",
    "                                delete_before_upload = False, \n",
    "                                silent_mode = True)\n",
    "        except:\n",
    "            break        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a list of featured words\n",
    "featuredwords = pd.DataFrame(list(wordfeatures_collection.find()))\n",
    "featuredwords = list(featuredwords.iloc[:,1].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training set for NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload to Mongo\n",
    "feature_dataset(train_set_collection,featuredwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set_prepared_NLTK = pd.DataFrame(list(featuring_temporary_collection.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data_mongoDb(train_set_collection_nltk,training_set_prepared_NLTK.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare training set for NLTK library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset(test_set_collection,featuredwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_prepared_NLTK = pd.DataFrame(list(featuring_temporary_collection.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x32178800>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# upload to Mongo\n",
    "upload_data_mongoDb(test_set_collection_nltk,test_set_prepared_NLTK.to_dict('records'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict with NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_for_nltk(collection):\n",
    "    data = []\n",
    "    dataset = []\n",
    "    \n",
    "    data = pd.DataFrame(list(collection.find()))\n",
    "\n",
    "    for index, row in data[['independent_variable_naive_bayes', 'dependent_variable']].iterrows():\n",
    "          dataset.append((row[0], row[1]))\n",
    "            \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = transform_data_for_nltk(train_set_collection_nltk)\n",
    "test_set = transform_data_for_nltk(test_set_collection_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8371904761904762\n"
     ]
    }
   ],
   "source": [
    "# create Naive Bayes classifier\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Test accuracy\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Try your own words\n",
    "print(classifier.classify(getFeatures('great'.split(),featuredwords)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Try your own words\n",
    "print(classifier.classify(getFeatures('bad'.split(),featuredwords)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy - Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  |     0     1 |\n",
      "--+-------------+\n",
      "0 | <2005> 2655 |\n",
      "1 |   764<15576>|\n",
      "--+-------------+\n",
      "(row = reference; col = test)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set_pred = [classifier.classify(word) for (word, tag) in test_set]\n",
    "test_set_tag = [tag for (word, tag) in test_set]\n",
    "\n",
    "\n",
    "print(nltk.ConfusionMatrix(test_set_tag, test_set_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix depicts that 2655 cases were predicted as false positives (I.order error). If we would predict all as positives, we would get 4670 cases estimated as false positives, therefore it would mean 77.7% model accuracy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting power of our Naive bayes model is 83.7%. If we wouldn't use any model and take into account that we have a small number of negative cases in our sample, therefore let's predict all as positive. We would get accuracy of 77.7%. Our model is better about 6%. We cannot consider this result as a success. Let's update our model with following steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use a different column such as 'review/text' and test whether productid or userid is relevant. \n",
    "2. In terms of featurewords, use only word that have biggest different between good and bad category.\n",
    "3. Use function collocations that can capture pair of words (see appendix).\n",
    "4. Use sklearn library and class NaiveBayes with bernouli distribution. \n",
    "5. Start with the sample and pick equal number of positive and negative cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collocations on all words in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Usage of Collocation in practise\n",
    "\n",
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(wordList)\n",
    "finder.nbest(bigram_measures.pmi, 1000)  # doctest: +NORMALIZE_WHITESPACE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
